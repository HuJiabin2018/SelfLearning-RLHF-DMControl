defaults:
  - agent: sac

# Basic setup
experiment: sac_self

# reward learning
segment: 50
activation: tanh
num_seed_steps: 1000
num_unsup_steps: 9000
num_interact: 5000
reward_lr: 0.0003
reward_batch: 128
reward_update: 200
feed_type: 1
reset_update: 100
topK: 5
ensemble_size: 3
max_feedback: 1400
large_batch: 10
label_margin: 0.0
teacher_beta: -1
teacher_gamma: 1
teacher_eps_mistake: 0
teacher_eps_skip: 0
teacher_eps_equal: 0

# scheduling
reward_schedule: 0

# training 
num_train_steps: 1e6
replay_buffer_capacity: ${num_train_steps}

# evaluation config
eval_frequency: 10000
num_eval_episodes: 10
device: cuda

# logger
log_frequency: 10000
log_save_tb: true

# video recorder
save_video: false
save_model: true

# setups
seed: 1

#env
env: dog_stand
gradient_update: 1

# Self-learning
self_frequency: 5000
VBU: 95
VBD: 80

# hydra configuration
hydra:
  name: ${env}
  run:
    dir: ./exp/${env}/H${diag_gaussian_actor.params.hidden_dim}_L${diag_gaussian_actor.params.hidden_depth}_B${agent.params.batch_size}_tau${agent.params.critic_tau}/${agent.name}_unsup${num_unsup_steps}_topk${topK}_${experiment}_lr${agent.params.actor_lr}_temp${agent.params.init_temperature}_seed${seed}
